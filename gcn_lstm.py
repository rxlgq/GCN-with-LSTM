# -*- coding: utf-8 -*-
"""GCN_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mD1dHbKJI9d-ThuA89AYpjAImbazLw8V
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
import numpy as np
import math

import networkx as nx

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

G = nx.karate_club_graph()

pos = nx.spring_layout(G)
color = []
for node in G.nodes:
    if G.nodes[node]['club'] == 'Mr. Hi':
        color.append('r')
    elif G.nodes[node]['club'] == 'Officer':
        color.append('b')

plt.figure(figsize=(5, 5))
#nx.draw(G, pos=pos, node_size=200, node_color=color, with_labels=True)
nx.draw_networkx(G, pos=pos, node_size=200, node_color=color, with_labels=True)

A = nx.adjacency_matrix(G).todense()
L = nx.laplacian_matrix(G).todense()
D = L + A

A_tilde = np.array(A).astype(np.float) + np.identity(G.number_of_nodes(), dtype=np.float)
D_temp = np.array(D).astype(np.float)   # <-- change to D_tilde = D + I

for i in range(G.number_of_nodes()):
  D_temp[i][i] = 1.0 / math.sqrt(D_temp[i][i])

A_hat =  np.matmul(np.matmul(D_temp, A_tilde), D_temp)

X = np.identity(G.number_of_nodes(), dtype=np.float)   

# (*)

y = []
for node in G.nodes:
    if G.nodes[node]['club'] == 'Mr. Hi':
        y.append(0)
    elif G.nodes[node]['club'] == 'Officer':
        y.append(1)

X.shape

class GCN(nn.Module):
  def __init__(self, A_hat, num_feat, num_hidden, num_class):
    super(GCN, self).__init__()
    self.num_feat = num_feat
    self.num_hidden = num_hidden
    self.num_class = num_class
    self.A_hat = A_hat
    self.W_0 = nn.Parameter(torch.Tensor(num_feat, num_hidden))
    self.W_1 = nn.Parameter(torch.Tensor(num_hidden, num_class))
    self.reset_parameters()

  def reset_parameters(self):
    stdv = 1. / math.sqrt(self.W_0.size(1))
    self.W_0.data.uniform_(-stdv, stdv)
    stdv = 1. / math.sqrt(self.W_1.size(1))
    self.W_1.data.uniform_(-stdv, stdv)

  def forward(self, X, A_hat):
    H = torch.mm(torch.mm(A_hat, X), self.W_0)
    H = F.relu(H)
    H = torch.mm(torch.mm(A_hat, H),self.W_1)
    return F.log_softmax(H, dim=1)

num_feat = len(G.nodes())
num_hidden = 10
num_class = 2

model = GCN(A_hat, num_feat, num_hidden, num_class).to(device)

#for p in model.parameters():
#  print(p.shape)   # 形の確認
#  print(p)   # 初期値の確認

loss_fun = nn.NLLLoss()
optimizer = optim.Adam(model.parameters())

#print(optimizer)   # デフォルト値の確認

A_hat_tensor = torch.Tensor(A_hat).to(device)
X_tensor = torch.Tensor(X).to(device)
y_tensor = torch.LongTensor(y).to(device)    

loss_hist = []
acc_hist = []

for epoch in range(500):
  model.train()
  model.zero_grad()

  output = model(X_tensor, A_hat_tensor)
  loss = loss_fun(output, y_tensor)
  loss_hist.append(loss.item())

  preds = torch.argmax(output, dim=1)
  acc = torch.mean(torch.eq(preds, y_tensor).type(torch.DoubleTensor)).numpy()
  acc_hist.append(acc)

  if (epoch+1)%10 == 0:
    print('{}  Loss: {:.4f}  Acc: {:.4f}'.format(epoch+1, loss.item(), acc))

  loss.backward()
  optimizer.step()

W_0 = nn.Parameter(torch.Tensor(num_feat, num_hidden))
H = torch.mm(torch.mm(A_hat_tensor, X_tensor), W_0)

lstm = nn.LSTM(num_feat, num_hidden)
input1 = X_tensor.view(len(X_tensor), 1, -1)
out1, out2 = lstm(input1)

out1.shape

H.shape

class GCN_LSTM(nn.Module):
  def __init__(self, A_hat, num_feat, num_hidden, num_class):
    super(GCN_LSTM, self).__init__()
    self.num_feat = num_feat
    self.num_hidden = num_hidden
    self.num_class = num_class
    self.A_hat = A_hat
    self.lstm = nn.LSTM(num_feat,num_feat)
    self.W_0 = nn.Parameter(torch.Tensor(num_feat, num_hidden))
    self.W_1 = nn.Parameter(torch.Tensor(num_hidden, num_class))
    self.reset_parameters()

  def reset_parameters(self):
    stdv = 1. / math.sqrt(self.W_0.size(1))
    self.W_0.data.uniform_(-stdv, stdv)
    stdv = 1. / math.sqrt(self.W_1.size(1))
    self.W_1.data.uniform_(-stdv, stdv)

  def forward(self, X, A_hat):
    
    lstm_input1 = X.view(len(X), 1, -1)
    X_out1, X_out2 = self.lstm(lstm_input1)
    X_out1      = X_out1.view(-1, self.num_feat)

    lstm_input2 = A_hat.view(len(A_hat), 1, -1)
    A_out1, A_out2 = self.lstm(lstm_input2)
    A_out1      = A_out1.view(-1, self.num_feat)

    H = torch.mm(torch.mm(A_out1, X_out1), self.W_0)
    H = F.relu(H)
    H = torch.mm(torch.mm(A_out1, H),self.W_1)
    return F.log_softmax(H, dim=1)

num_feat = len(G.nodes())
num_hidden = 10
num_class = 2

model = GCN_LSTM(A_hat, num_feat, num_hidden, num_class).to(device)

#for p in model.parameters():
#  print(p.shape)   # 形の確認
#  print(p)   # 初期値の確認

loss_fun = nn.NLLLoss()
optimizer = optim.Adam(model.parameters())

#print(optimizer)   # デフォルト値の確認

A_hat_tensor = torch.Tensor(A_hat).to(device)
X_tensor = torch.Tensor(X).to(device)
y_tensor = torch.LongTensor(y).to(device)    

l_loss_hist = []
l_acc_hist = []

for epoch in range(500):
  model.train()
  model.zero_grad()

  output = model(X_tensor, A_hat_tensor)
  loss = loss_fun(output, y_tensor)
  l_loss_hist.append(loss.item())

  preds = torch.argmax(output, dim=1)
  acc = torch.mean(torch.eq(preds, y_tensor).type(torch.DoubleTensor)).numpy()
  l_acc_hist.append(acc)

  if (epoch+1)%10 == 0:
    print('{}  Loss: {:.4f}  Acc: {:.4f}'.format(epoch+1, loss.item(), acc))

  loss.backward()
  optimizer.step()

#for p in model.parameters():
#  print(p)   # 学習後の値の確認
plt.figure(figsize=(5, 3))
plt.plot(loss_hist, label="loss_GCN")
plt.plot(l_loss_hist, label="loss_GCN_LSTM")
plt.plot(acc_hist, label="acc_GCN")
plt.plot(l_acc_hist, label="acc_GCN_LSTM")
plt.legend()
plt.grid(True)
plt.savefig("GCN_lstm_loss.png")
plt.show()

